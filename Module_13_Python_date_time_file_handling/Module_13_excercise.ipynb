{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#  Module 13 Assigment: Level 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’» Python Datetime Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the current day, month, year, hour, minute and timestamp from datetime module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 11:42:51.025500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(now)                      # 2021-07-08 07:34:46.549883\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the current date using this format: \"%m/%d/%Y, %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date & time: 01/22/2025, 11:46:05\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# current date and time\n",
    "now = datetime.now()\n",
    "time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "# mm/dd/YY H:M:S format\n",
    "print(\"Date & time:\", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Today is 5 December, 2019. Change this time string to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date_string = \"5 December, 2019\"\n",
    "\n",
    "date_object = datetime.strptime(date_string, \"%d %B, %Y\")\n",
    "\n",
    "print(date_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate the time difference between now and new year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference between now and new year: 346 days\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.strptime(\"20 January, 2025\", \"%d %B, %Y\")\n",
    "\n",
    "new_year_date = datetime.strptime(\"1 January, 2026\", \"%d %B, %Y\")\n",
    "\n",
    "time_difference = new_year_date - current_date\n",
    "\n",
    "print(f\"Time difference between now and new year: {time_difference.days} days\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate the time difference between 1 January 1970 and now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference between 1970 and now: 20110 days\n"
     ]
    }
   ],
   "source": [
    "old_date = datetime.strptime(\"1 January, 1970\", \"%d %B, %Y\")\n",
    "\n",
    "current_date = datetime.now()\n",
    "time_difference = current_date - old_date\n",
    "\n",
    "print(f\"Time difference between 1970 and now: {time_difference.days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Think, what can you use the datetime module for? Examples:\n",
    "- ### Time series analysis\n",
    "- ### To get a timestamp of any activities in an application\n",
    "- ### Adding posts on a blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datetime` module is a powerful tool in Python for working with dates and times. Here are various practical use cases beyond the examples you provided:\n",
    "\n",
    "### 1. **Time Series Analysis**\n",
    "   - **Use Case**: Analyzing stock prices, weather data, or sensor readings over time.\n",
    "   - **Example**: Using `datetime` to parse time data and index data frames for financial analysis.\n",
    "\n",
    "### 2. **Timestamps for Logging Activities**\n",
    "   - **Use Case**: Tracking user actions, system events, or debugging logs.\n",
    "   - **Example**:  \n",
    "     ```python\n",
    "     from datetime import datetime\n",
    "     log_time = datetime.now()\n",
    "     print(f\"User logged in at {log_time}\")\n",
    "     ```\n",
    "\n",
    "### 3. **Scheduling and Notifications**\n",
    "   - **Use Case**: Building alarms, reminders, or task schedulers.\n",
    "   - **Example**: Checking if a task is due and sending a reminder.\n",
    "\n",
    "### 4. **Data Expiration and Validity Checking**\n",
    "   - **Use Case**: Managing limited-time offers, subscriptions, or access tokens.\n",
    "   - **Example**: Comparing the current date with an expiry date.\n",
    "\n",
    "### 5. **Calculating Age or Duration**\n",
    "   - **Use Case**: Finding the age of a person or duration of an event.\n",
    "   - **Example**:  \n",
    "     ```python\n",
    "     birth_date = datetime(2000, 5, 10)\n",
    "     current_date = datetime.now()\n",
    "     age = (current_date - birth_date).days // 365\n",
    "     print(f\"Age: {age} years\")\n",
    "     ```\n",
    "\n",
    "### 6. **Date-Based Filenames or Backup Systems**\n",
    "   - **Use Case**: Naming backups or logs with timestamps.\n",
    "   - **Example**:  \n",
    "     ```python\n",
    "     filename = f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "     ```\n",
    "\n",
    "### 7. **Expiration Reminder for Online Stores**\n",
    "   - **Use Case**: Sending notifications for cart expiration or flash sales.\n",
    "\n",
    "### 8. **Analytics on Blog or Website Posts**\n",
    "   - **Use Case**: Sorting and filtering posts by date, or finding the most popular post in a time frame.\n",
    "\n",
    "### 9. **Event Countdown or Progress Tracking**\n",
    "   - **Use Case**: Countdown to a product launch, event, or holiday.\n",
    "\n",
    "### 10. **Time Zone and Localization Handling**\n",
    "   - **Use Case**: Supporting users from different time zones by converting to local time using `pytz`.\n",
    "\n",
    "These examples demonstrate how `datetime` empowers applications to manage and manipulate time effectively, making it crucial for real-world programming solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’» File Handling Exercises:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a function which count number of lines and number of words in a text. All the files are in the data the folder: a) Read obama_speech.txt file and count number of lines and words b) Read michelle_obama_speech.txt file and count number of lines and words c) Read donald_speech.txt file and count number of lines and words d) Read melina_trump_speech.txt file and count number of lines and words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: obama_speech.txt\n",
      "Number of lines: 66\n",
      "Number of words: 2400\n",
      "\n",
      "File: michelle_obama_speech.txt\n",
      "Number of lines: 83\n",
      "Number of words: 2204\n",
      "\n",
      "File: donald_speech.txt\n",
      "Number of lines: 48\n",
      "Number of words: 1259\n",
      "\n",
      "File: melina_trump_speech.txt\n",
      "Number of lines: 33\n",
      "Number of words: 1375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_lines_and_words(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            number_of_lines = len(lines)\n",
    "            number_of_words = sum(len(line.split()) for line in lines)\n",
    "            return number_of_lines, number_of_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return 0, 0\n",
    "\n",
    "files = [\n",
    "    \"data/obama_speech.txt\",\n",
    "    \"data/michelle_obama_speech.txt\",\n",
    "    \"data/donald_speech.txt\",\n",
    "    \"data/melina_trump_speech.txt\"\n",
    "]\n",
    "\n",
    "for file_path in files:\n",
    "    lines, words = count_lines_and_words(file_path)\n",
    "    print(f\"File: {file_path[5:]}\")\n",
    "    print(f\"Number of lines: {lines}\")\n",
    "    print(f\"Number of words: {words}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the countries_data.json data file in data directory, create a function that finds the ten most spoken languages\n",
    "```python\n",
    "# Your output should look like this\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', 10))\n",
    "[(91, 'English'),\n",
    "(45, 'French'),\n",
    "(25, 'Arabic'),\n",
    "(24, 'Spanish'),\n",
    "(9, 'Russian'),\n",
    "(9, 'Portuguese'),\n",
    "(8, 'Dutch'),\n",
    "(7, 'German'),\n",
    "(5, 'Chinese'),\n",
    "(4, 'Swahili'),\n",
    "(4, 'Serbian')]\n",
    "\n",
    "# Your output should look like this\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', 3))\n",
    "[(91, 'English'),\n",
    "(45, 'French'),\n",
    "(25, 'Arabic')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(91, 'English'), (45, 'French'), (25, 'Arabic'), (24, 'Spanish'), (9, 'Portuguese'), (9, 'Russian'), (8, 'Dutch'), (7, 'German'), (5, 'Chinese'), (4, 'Serbian')]\n",
      "[(91, 'English'), (45, 'French'), (25, 'Arabic')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def most_spoken_languages(filename, top_n):\n",
    "    with open(filename, 'r') as file:\n",
    "        countries_data = json.load(file)\n",
    "    languages = []\n",
    "\n",
    "    for country in countries_data:\n",
    "        languages.extend(country.get('languages', []))\n",
    "\n",
    "    language_count = Counter(languages)\n",
    "\n",
    "    most_common_languages = language_count.most_common(top_n)\n",
    "\n",
    "    return [(count, language) for language, count in most_common_languages]\n",
    "\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=10))\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the countries_data.json data file in data directory, create a function that creates a list of the ten most populated countries\n",
    "\n",
    "```python\n",
    "# Your output should look like this\n",
    "print(most_populated_countries(filename='./data/countries_data.json', 10))\n",
    "\n",
    "[\n",
    "{'country': 'China', 'population': 1377422166},\n",
    "{'country': 'India', 'population': 1295210000},\n",
    "{'country': 'United States of America', 'population': 323947000},\n",
    "{'country': 'Indonesia', 'population': 258705000},\n",
    "{'country': 'Brazil', 'population': 206135893},\n",
    "{'country': 'Pakistan', 'population': 194125062},\n",
    "{'country': 'Nigeria', 'population': 186988000},\n",
    "{'country': 'Bangladesh', 'population': 161006790},\n",
    "{'country': 'Russian Federation', 'population': 146599183},\n",
    "{'country': 'Japan', 'population': 126960000}\n",
    "]\n",
    "\n",
    "# Your output should look like this\n",
    "\n",
    "print(most_populated_countries(filename='./data/countries_data.json', 3))\n",
    "[\n",
    "{'country': 'China', 'population': 1377422166},\n",
    "{'country': 'India', 'population': 1295210000},\n",
    "{'country': 'United States of America', 'population': 323947000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}, {'country': 'Indonesia', 'population': 258705000}, {'country': 'Brazil', 'population': 206135893}, {'country': 'Pakistan', 'population': 194125062}, {'country': 'Nigeria', 'population': 186988000}, {'country': 'Bangladesh', 'population': 161006790}, {'country': 'Russian Federation', 'population': 146599183}, {'country': 'Japan', 'population': 126960000}]\n",
      "[{'country': 'China', 'population': 1377422166}, {'country': 'India', 'population': 1295210000}, {'country': 'United States of America', 'population': 323947000}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def most_populated_countries(filename, n):\n",
    "    # Read the JSON file\n",
    "    with open(filename, 'r') as file:\n",
    "        countries_data = json.load(file)\n",
    "    \n",
    "    # Sort countries by population in descending order\n",
    "    sorted_countries = sorted(\n",
    "        countries_data, key=lambda country: country['population'], reverse=True\n",
    "    )\n",
    "    \n",
    "    # Get the top n populated countries\n",
    "    top_countries = sorted_countries[:n]\n",
    "    \n",
    "    # Create the desired output format\n",
    "    result = [{'country': country['name'], 'population': country['population']} for country in top_countries]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(most_populated_countries(filename='./data/countries_data.json', n=10))\n",
    "print(most_populated_countries(filename='./data/countries_data.json', n=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Level 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract all incoming email addresses as a list from the email_exchange_big.txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/email_exchange_big.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_emails\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m emails \u001b[38;5;241m=\u001b[39m extract_emails(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/email_exchange_big.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracted email addresses:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m emails:\n",
      "Cell \u001b[1;32mIn[65], line 8\u001b[0m, in \u001b[0;36mextract_emails\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      5\u001b[0m email_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.-]+@[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.-]+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read file and extract emails\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     10\u001b[0m     emails \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(email_pattern, content)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/email_exchange_big.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(filename):\n",
    "    # Regex pattern for email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "    \n",
    "    # Read file and extract emails\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        emails = re.findall(email_pattern, content)\n",
    "    \n",
    "    # Remove duplicates while maintaining order\n",
    "    unique_emails = list(dict.fromkeys(emails))\n",
    "    return unique_emails\n",
    "\n",
    "# Example usage\n",
    "emails = extract_emails(filename='./data/email_exchange_big.txt')\n",
    "print(\"\\nExtracted email addresses:\")\n",
    "for email in emails:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find the most common words in the English language. Call the name of your function find_most_common_words, it will take two parameters - a string or a file and a positive integer, indicating the number of words. Your function will return an array of tuples in descending order. Check the output\n",
    "```python\n",
    "    # Your output should look like this\n",
    "    print(find_most_common_words('sample.txt', 10))\n",
    "    [(10, 'the'),\n",
    "    (8, 'be'),\n",
    "    (6, 'to'),\n",
    "    (6, 'of'),\n",
    "    (5, 'and'),\n",
    "    (4, 'a'),\n",
    "    (4, 'in'),\n",
    "    (3, 'that'),\n",
    "    (2, 'have'),\n",
    "    (2, 'I')]\n",
    "\n",
    "    # Your output should look like this\n",
    "    print(find_most_common_words('sample.txt', 5))\n",
    "\n",
    "    [(10, 'the'),\n",
    "    (8, 'be'),\n",
    "    (6, 'to'),\n",
    "    (6, 'of'),\n",
    "    (5, 'and')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 'the'), (15, 'to'), (15, 'of'), (12, 'and'), (7, 'that')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(find_most_common_words('sample.txt', 5)) # sample.txt is existing file available in the data folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "### 6. Use the function, find_most_frequent_words to find: a) The ten most frequent words used in Obama's speech b) The ten most frequent words used in Michelle's speech c) The ten most frequent words used in Trump's speech d) The ten most frequent words used in Melina's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most frequent words used in Obama's speech: [(129, 'the'), (113, 'and'), (81, 'of'), (70, 'to'), (67, 'our'), (62, 'we'), (50, 'that'), (48, 'a'), (36, 'is'), (25, 'in')]\n",
      "The ten most frequent words used in Michelle's speech: [(96, 'and'), (85, 'the'), (84, 'to'), (50, 'that'), (46, 'of'), (41, 'a'), (37, 'he'), (36, 'in'), (28, 'my'), (28, 'i')]\n",
      "The ten most frequent words used in Melina's speech: [(77, 'and'), (55, 'to'), (52, 'the'), (29, 'is'), (28, 'i'), (27, 'for'), (25, 'of'), (24, 'that'), (22, 'a'), (21, 'you')]\n",
      "The ten most frequent words used in Trump's speech: [(65, 'the'), (59, 'and'), (44, 'we'), (40, 'will'), (38, 'of'), (32, 'to'), (30, 'our'), (20, 'is'), (17, 'america'), (13, 'for')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(f\"The ten most frequent words used in Obama's speech: {find_most_common_words('data/obama_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Michelle's speech: {find_most_common_words('data/michelle_obama_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Melina's speech: {find_most_common_words('data/melina_trump_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Trump's speech: {find_most_common_words('data/donald_speech.txt', 10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write a python application that checks similarity between two texts. It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. For instance check the similarity between the transcripts of Michelle's and Melina's speech. You may need a couple of functions, function to clean the text(clean_text), function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). List of stop words are in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'module' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jaccard_similarity(text1, text2)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m similarity \u001b[38;5;241m=\u001b[39m process_text(file1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/melina_trump_speech.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, file2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/michelle_obama_speech.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJaccard similarity score between the two speeches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 31\u001b[0m, in \u001b[0;36mprocess_text\u001b[1;34m(file1, file2)\u001b[0m\n\u001b[0;32m     28\u001b[0m     text2 \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Clean the text and remove stop words, returning word lists\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m text1 \u001b[38;5;241m=\u001b[39m remove_support_words(clean_text(text1))\n\u001b[0;32m     32\u001b[0m text2 \u001b[38;5;241m=\u001b[39m remove_support_words(clean_text(text2))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard similarity using word lists\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[93], line 13\u001b[0m, in \u001b[0;36mremove_support_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove stop words from the text using the imported stop_words list.\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m---> 13\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_words\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'module' is not iterable"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from Data import stop_words  # Import stop words from the stop_words.py file\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the input text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def remove_support_words(text):\n",
    "    \"\"\"Remove stop words from the text using the imported stop_words list.\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two lists.\"\"\"\n",
    "    intersection = len(set(list1) & set(list2))\n",
    "    union = len(set(list1) | set(list2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def process_text(file1, file2):\n",
    "    \"\"\"Process the input text files, clean them, and remove stop words.\"\"\"\n",
    "    # Read the text files\n",
    "    with open(file1, 'r') as f:\n",
    "        text1 = f.read()\n",
    "    with open(file2, 'r') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    # Clean the text and remove stop words, returning word lists\n",
    "    text1 = remove_support_words(clean_text(text1))\n",
    "    text2 = remove_support_words(clean_text(text2))\n",
    "\n",
    "    # Calculate Jaccard similarity using word lists\n",
    "    return jaccard_similarity(text1, text2)\n",
    "\n",
    "# Example usage:\n",
    "similarity = process_text(file1='data/melina_trump_speech.txt', file2='data/michelle_obama_speech.txt')\n",
    "print(f\"Jaccard similarity score between the two speeches: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Find the 10 most repeated words in the romeo_and_juliet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ten most repeated word in romeo_and_juliet.txt: [(868, 'the'), (800, 'and'), (661, 'to'), (658, 'i'), (535, 'of'), (530, 'a'), (381, 'is'), (378, 'in'), (371, 'that'), (367, 'you')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(f\"The ten most repeated word in romeo_and_juliet.txt: {find_most_common_words('data/romeo_and_juliet.txt', 10)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Read the hacker news csv file and find out: a) Count the number of lines containing python or Python b) Count the number lines containing JavaScript, javascript or Javascript c) Count the number lines containing Java and not JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of lines containing 'python' or 'Python': 0\n",
      "Count of lines containing 'JavaScript', 'javascript', or 'Javascript': 0\n",
      "Count of lines containing 'Java' but not 'JavaScript': 0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def count_keywords_in_csv(file_path):\n",
    "    # Initialize counters\n",
    "    count_python = 0\n",
    "    count_javascript = 0\n",
    "    count_java_not_javascript = 0\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        \n",
    "        # Assuming the title/content is in the first column (index 0)\n",
    "        for row in reader:\n",
    "            if len(row) > 0:  # Check if the row has content\n",
    "                text = row[0].lower()  # Assuming the content to check is in the first column\n",
    "                # Check for Python-related occurrences\n",
    "                if 'python' in text:\n",
    "                    count_python += 1\n",
    "                # Check for JavaScript-related occurrences\n",
    "                if 'javascript' in text:\n",
    "                    count_javascript += 1\n",
    "                # Check for Java but not JavaScript\n",
    "                if 'java' in text and 'javascript' not in text:\n",
    "                    count_java_not_javascript += 1\n",
    "\n",
    "    return count_python, count_javascript, count_java_not_javascript\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'data/hacker_news.csv'  # Ensure this is the correct file path\n",
    "python_count, javascript_count, java_not_js_count = count_keywords_in_csv(file_path)\n",
    "\n",
    "print(f\"Count of lines containing 'python' or 'Python': {python_count}\")\n",
    "print(f\"Count of lines containing 'JavaScript', 'javascript', or 'Javascript': {javascript_count}\")\n",
    "print(f\"Count of lines containing 'Java' but not 'JavaScript': {java_not_js_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
